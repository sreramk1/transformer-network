# Here is why the transformer network's positional embeddings and encodings are a hack

Explains why the positional embedding and encoding feels like a hack

$f(x)$
